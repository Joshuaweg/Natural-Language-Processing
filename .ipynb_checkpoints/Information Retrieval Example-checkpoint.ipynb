{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8132539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n",
      " 18 Editions of the Dewey Decimal Classifications Comaromi, J.P. The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad. \n"
     ]
    }
   ],
   "source": [
    "def read_documents():\n",
    "    f = open(\"cisi/CISI.ALL\")\n",
    "    merged = \"\"\n",
    "    for a_line in f.readlines():\n",
    "        if a_line.startswith(\".\"):\n",
    "            merged += \"\\n\"+a_line.strip()\n",
    "        else:\n",
    "            merged += \" \" + a_line.strip()\n",
    "    documents={}\n",
    "    \n",
    "    content = \"\"\n",
    "    doc_id = \"\"\n",
    "    \n",
    "    for a_line in merged.split(\"\\n\"):\n",
    "        if a_line.startswith(\".I\"):\n",
    "            doc_id = a_line.split(\" \")[1].strip()\n",
    "        elif a_line.startswith(\".X\"):\n",
    "            documents[doc_id] = content\n",
    "            content = \"\"\n",
    "            doc_id = \"\"\n",
    "        else:\n",
    "            content += a_line.strip()[3:]+\" \"\n",
    "    f.close()\n",
    "    return documents\n",
    "\n",
    "documents = read_documents()\n",
    "print(len(documents))\n",
    "print(documents.get(\"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f090d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles? \n"
     ]
    }
   ],
   "source": [
    "def read_queries():\n",
    "    f=open(\"cisi/CISI.QRY\")\n",
    "    merged = \"\"\n",
    "    \n",
    "    for a_line in f.readlines():\n",
    "        if a_line.startswith(\".\"):\n",
    "            merged+=\"\\n\"+a_line.strip()\n",
    "        else:\n",
    "            merged+= \" \"+a_line.strip()\n",
    "            \n",
    "    queries = {}\n",
    "    \n",
    "    content = \"\"\n",
    "    qry_id = \"\"\n",
    "    \n",
    "    for a_line in merged.split(\"\\n\"):\n",
    "        if a_line.startswith(\".I\"):\n",
    "            if not content==\"\":\n",
    "                queries[qry_id] = content\n",
    "                content = \"\"\n",
    "                qry_id = \"\"\n",
    "            qry_id = a_line.split(\" \")[1].strip()\n",
    "        elif a_line.startswith(\".W\") or a_line.startswith(\".T\"):\n",
    "            content += a_line.strip()[3:] + \" \"\n",
    "    queries[qry_id] = content\n",
    "    f.close()\n",
    "    return queries\n",
    "\n",
    "queries = read_queries()\n",
    "print(len(queries))\n",
    "print(queries.get(\"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76dcb3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '37', '39', '41', '42', '43', '44', '45', '46', '49', '50', '52', '54', '55', '56', '57', '58', '61', '62', '65', '66', '67', '69', '71', '76', '79', '81', '82', '84', '90', '92', '95', '96', '97', '98', '99', '100', '101', '102', '104', '109', '111'])\n",
      "['28', '35', '38', '42', '43', '52', '65', '76', '86', '150', '189', '192', '193', '195', '215', '269', '291', '320', '429', '465', '466', '482', '483', '510', '524', '541', '576', '582', '589', '603', '650', '680', '711', '722', '726', '783', '813', '820', '868', '869', '894', '1162', '1164', '1195', '1196', '1281']\n"
     ]
    }
   ],
   "source": [
    "def read_mappings():\n",
    "    f=open(\"cisi/CISI.REL\")\n",
    "            \n",
    "    mappings = {}\n",
    "    for a_line in f.readlines():\n",
    "        voc = a_line.strip().split()\n",
    "        key = voc[0].strip()\n",
    "        current_value = voc[1].strip()\n",
    "        value=[]\n",
    "        if key in mappings.keys():\n",
    "            value = mappings.get(key)\n",
    "        value.append(current_value)\n",
    "        mappings[key] = value\n",
    "    \n",
    "    f.close()\n",
    "    return mappings\n",
    "    \n",
    "\n",
    "mappings = read_mappings()\n",
    "print(len(mappings))\n",
    "print(mappings.keys())\n",
    "print(mappings.get(\"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ef91004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joshu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n",
      "{'18': 1, 'edit': 4, 'dewey': 3, 'decim': 2, 'class': 2, 'comarom': 1, 'j.p.': 1, 'pres': 1, 'study': 1, 'hist': 2, 'first': 2, 'ddc': 2, 'publ': 1, '1876': 1, 'eighteen': 1, '1971': 1, 'fut': 1, 'continu': 1, 'appear': 1, 'nee': 1, 'spit': 1, \"'s\": 1, 'long': 1, 'healthy': 1, 'lif': 1, 'howev': 1, 'ful': 1, 'story': 1, 'nev': 1, 'told': 1, 'biograph': 1, 'brief': 1, 'describ': 1, 'system': 1, 'attempt': 1, 'provid': 1, 'detail': 1, 'work': 1, 'spur': 1, 'grow': 1, 'libr': 1, 'country': 1, 'abroad': 1}\n",
      "43\n",
      "112\n",
      "['problem', 'concern', 'mak', 'describ', 'titl', 'difficul', 'involv', 'autom', 'retriev', 'artic', 'approxim', 'titl', 'us', 'relev', 'cont', 'artic', 'titl']\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def get_words(text):\n",
    "    stoplist = set(stopwords.words(\"english\"))\n",
    "    st = LancasterStemmer()\n",
    "    word_list = [st.stem(word) for word in word_tokenize(text.lower()) if not word in stoplist and not word in string.punctuation]\n",
    "    return word_list\n",
    "def get_terms(text):\n",
    "    stoplist = set(stopwords.words(\"english\"))\n",
    "    terms = {}\n",
    "    st = LancasterStemmer()\n",
    "    word_list = [st.stem(word) for word in word_tokenize(text.lower()) if not word in stoplist and not word in string.punctuation]\n",
    "    for word in word_list:\n",
    "        terms[word] = terms.get(word,0)+1\n",
    "    return terms\n",
    "\n",
    "doc_words = {}\n",
    "query_words = {}\n",
    "doc_terms = {}\n",
    "query_terms = {}\n",
    "for doc_id in documents.keys():\n",
    "    doc_words[doc_id] = get_words(documents.get(doc_id))\n",
    "for qry_id in queries.keys():\n",
    "    query_words[qry_id] = get_words(queries.get(qry_id))\n",
    "for doc_id in documents.keys():\n",
    "    doc_terms[doc_id] = get_terms(documents.get(doc_id))\n",
    "for qry_id in queries.keys():\n",
    "    query_terms[qry_id] = get_terms(queries.get(qry_id))\n",
    "print(len(doc_terms))\n",
    "print(doc_terms.get(\"1\"))\n",
    "print(len(doc_terms.get(\"1\")))\n",
    "print(len(query_words))\n",
    "print(query_words.get(\"1\"))\n",
    "print(len(query_words.get(\"1\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "852888fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '3', '4', '5', '6', '9', '10', '12', '15', '16', '17', '18', '19', '22', '23', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '37', '39', '40', '41', '42', '44', '46', '47', '48', '49', '51', '52', '53', '54', '57', '58', '59', '60', '62', '63', '64', '66', '67', '68', '71', '72', '73', '74', '78', '79', '80', '81', '82', '84', '85', '88', '89', '90', '93', '95', '96', '97', '98', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '116', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133']\n",
      "990\n"
     ]
    }
   ],
   "source": [
    "def retrieve_documents(doc_words,query):\n",
    "    docs=[]\n",
    "    for doc_id in doc_words.keys():\n",
    "        found = False\n",
    "        i = 0\n",
    "        while i<len(query) and not found:\n",
    "            word = query[i]\n",
    "            if word in doc_words.get(doc_id):\n",
    "                docs.append(doc_id)\n",
    "                found=True\n",
    "            else:\n",
    "                i+=1\n",
    "    return docs\n",
    "\n",
    "docs = retrieve_documents(doc_words,query_words.get(\"3\"))\n",
    "\n",
    "print(docs[:100])\n",
    "print(len(docs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
